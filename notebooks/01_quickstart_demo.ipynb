{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SereneSense Quickstart Demo\n",
        "\n",
        "Welcome to SereneSense! This notebook demonstrates the basic workflow for detecting military vehicle sounds using a pre-trained model.\n",
        "\n",
        "**Duration**: ~5 minutes\n",
        "**Topics**: Model loading, inference, visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation Check\n",
        "\n",
        "First, let's verify all required packages are installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Verify packages\n",
        "packages = ['torch', 'torchaudio', 'librosa', 'soundfile', 'plotly', 'numpy']\n",
        "missing = []\n",
        "\n",
        "for package in packages:\n",
        "    try:\n",
        "        __import__(package)\n",
        "        print(f'\u2713 {package} installed')\n",
        "    except ImportError:\n",
        "        missing.append(package)\n",
        "        print(f'\u2717 {package} NOT installed')\n",
        "\n",
        "if missing:\n",
        "    print(f'\\nInstalling missing packages: {missing}')\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install'] + missing)\n",
        "    print('Installation complete!')\n",
        "else:\n",
        "    print('\\n\u2713 All packages installed!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print('\u2713 All imports successful')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Pre-trained Model\n",
        "\n",
        "Load the AudioMAE model for military vehicle sound detection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.core.models.audioMAE.model import AudioMAE, AudioMAEConfig\n",
        "from src.core.core.model_manager import ModelManager\n",
        "\n",
        "# Initialize model\n",
        "model_manager = ModelManager(model_type='audioMAE')\n",
        "print(f'\u2713 Model loaded: {model_manager.model_type}')\n",
        "print(f'  Device: {model_manager.device}')\n",
        "print(f'  Model size: {sum(p.numel() for p in model_manager.model.parameters()) / 1e6:.1f}M parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Sample Audio\n",
        "\n",
        "Since we don't have a real audio file, let's create a synthetic audio sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic audio for demonstration\n",
        "sr = 16000  # Sample rate\n",
        "duration = 5  # 5 seconds\n",
        "t = np.linspace(0, duration, int(sr * duration))\n",
        "\n",
        "# Create a synthetic signal with multiple frequencies (simulating vehicle sound)\n",
        "frequencies = [100, 250, 500, 1000]  # Multiple harmonics\n",
        "amplitudes = [0.3, 0.2, 0.15, 0.1]\n",
        "audio = np.zeros_like(t)\n",
        "\n",
        "for freq, amp in zip(frequencies, amplitudes):\n",
        "    audio += amp * np.sin(2 * np.pi * freq * t)\n",
        "\n",
        "# Add some noise\n",
        "audio += 0.05 * np.random.randn(len(audio))\n",
        "\n",
        "# Normalize\n",
        "audio = audio / np.max(np.abs(audio)) * 0.95\n",
        "\n",
        "print(f'\u2713 Created synthetic audio')\n",
        "print(f'  Sample rate: {sr} Hz')\n",
        "print(f'  Duration: {len(audio) / sr:.2f} seconds')\n",
        "print(f'  Audio shape: {audio.shape}')\n",
        "print(f'  Audio range: [{audio.min():.3f}, {audio.max():.3f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Audio\n",
        "\n",
        "Let's visualize the waveform and spectrogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create figure with subplots\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "# Waveform\n",
        "axes[0].plot(t, audio, linewidth=0.5)\n",
        "axes[0].set_xlabel('Time (s)')\n",
        "axes[0].set_ylabel('Amplitude')\n",
        "axes[0].set_title('Waveform')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Spectrogram (Mel scale)\n",
        "mel_spec = librosa.feature.melspectrogram(\n",
        "    y=audio, sr=sr, n_mels=64, fmax=8000\n",
        ")\n",
        "mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "im = axes[1].imshow(\n",
        "    mel_spec_db, aspect='auto', origin='lower', cmap='viridis',\n",
        "    extent=[0, duration, 0, 8000], interpolation='bilinear'\n",
        ")\n",
        "axes[1].set_xlabel('Time (s)')\n",
        "axes[1].set_ylabel('Frequency (Hz)')\n",
        "axes[1].set_title('Mel-Scale Spectrogram')\n",
        "plt.colorbar(im, ax=axes[1], label='Power (dB)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\u2713 Visualizations created')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Inference\n",
        "\n",
        "Process the audio through the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare audio for model\n",
        "# Model expects input shape: (batch_size, num_samples)\n",
        "audio_tensor = torch.FloatTensor(audio).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    model_manager.model.eval()\n",
        "    if torch.cuda.is_available():\n",
        "        audio_tensor = audio_tensor.to(model_manager.device)\n",
        "    \n",
        "    # Get model predictions\n",
        "    outputs = model_manager.model(audio_tensor)\n",
        "\n",
        "print('\u2713 Inference complete')\n",
        "print(f'  Output shape: {outputs.shape}')\n",
        "print(f'  Output type: {type(outputs)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Results\n",
        "\n",
        "Show the detection results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define class labels (from MAD dataset)\n",
        "class_labels = {\n",
        "    0: 'Helicopter',\n",
        "    1: 'Fighter Aircraft',\n",
        "    2: 'Military Vehicle',\n",
        "    3: 'Truck',\n",
        "    4: 'Footsteps',\n",
        "    5: 'Speech',\n",
        "    6: 'Background'\n",
        "}\n",
        "\n",
        "# Get predictions\n",
        "with torch.no_grad():\n",
        "    logits = outputs\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    top_prob, top_class = torch.max(probabilities, dim=1)\n",
        "\n",
        "predicted_class = top_class.item()\n",
        "confidence = top_prob.item()\n",
        "predicted_label = class_labels[predicted_class]\n",
        "\n",
        "print(f'\ud83c\udfaf Detection Results:')\n",
        "print(f'   Predicted Class: {predicted_label} (ID: {predicted_class})')\n",
        "print(f'   Confidence: {confidence * 100:.2f}%')\n",
        "print()\n",
        "print(f'\ud83d\udcca All Class Probabilities:')\n",
        "for i in range(len(class_labels)):\n",
        "    prob = probabilities[0, i].item()\n",
        "    print(f'   {class_labels[i]:20s}: {prob * 100:6.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Predictions\n",
        "\n",
        "Create an interactive visualization of the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract probabilities\n",
        "probs_np = probabilities[0].cpu().numpy()\n",
        "labels = [class_labels[i] for i in range(len(class_labels))]\n",
        "\n",
        "# Create interactive bar chart\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(\n",
        "        x=labels,\n",
        "        y=probs_np * 100,\n",
        "        marker=dict(\n",
        "            color=probs_np,\n",
        "            colorscale='Viridis',\n",
        "            showscale=True,\n",
        "            colorbar=dict(title='Probability (%)')\n",
        "        ),\n",
        "        text=[f'{p*100:.1f}%' for p in probs_np],\n",
        "        textposition='auto',\n",
        "    )\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='SereneSense: Military Vehicle Sound Detection',\n",
        "    xaxis_title='Sound Class',\n",
        "    yaxis_title='Probability (%)',\n",
        "    height=500,\n",
        "    hovermode='x unified',\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "Congratulations! You've successfully run inference with SereneSense. Here are some things you can try next:\n",
        "\n",
        "1. **Load Real Audio**: Replace the synthetic audio with your own WAV/MP3 files\n",
        "2. **Batch Processing**: Process multiple audio files at once\n",
        "3. **Real-time Detection**: See `05_deployment_walkthrough.ipynb` for real-time streaming\n",
        "4. **Model Training**: See `03_model_training.ipynb` to train your own model\n",
        "5. **Edge Optimization**: See `04_edge_optimization.ipynb` to optimize for edge devices\n",
        "\n",
        "For more details, check out the [Documentation](../docs/) and [GitHub Repository](https://github.com/serenesense/serenesense)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}