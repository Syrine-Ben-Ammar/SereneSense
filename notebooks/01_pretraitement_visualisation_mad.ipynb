{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1 : Pr√©traitement et Visualisation des Donn√©es MAD\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "\n",
    "1. [Introduction et Contexte](#1-introduction)\n",
    "2. [T√©l√©chargement du Dataset MAD](#2-telechargement)\n",
    "3. [Analyse Exploratoire Initiale](#3-analyse-exploratoire)\n",
    "4. [Pipeline de Pr√©traitement Audio](#4-pipeline-pretraitement)\n",
    "5. [Extraction des Features](#5-extraction-features)\n",
    "6. [Division Stratifi√©e du Dataset](#6-division-dataset)\n",
    "7. [Visualisations Finales](#7-visualisations-finales)\n",
    "8. [Conclusion](#8-conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et Contexte {#1-introduction}\n",
    "\n",
    "### Objectif du Projet SereneSense\n",
    "\n",
    "Ce notebook documente la premi√®re √©tape cruciale du projet **SereneSense** : le pr√©traitement et la visualisation des donn√©es audio du dataset **MAD (Military Audio Detection)**.\n",
    "\n",
    "### Le Dataset MAD\n",
    "\n",
    "- **Source** : Kaggle Hub (`junewookim/mad-dataset-military-audio-dataset`)\n",
    "- **Taille** : 7,466 √©chantillons audio\n",
    "- **Format** : Fichiers WAV (16-bit, 16kHz)\n",
    "- **Volume** : ~2.8GB compress√©\n",
    "- **Classes** : 7 cat√©gories de v√©hicules militaires\n",
    "\n",
    "### Les 7 Classes de Sons Militaires\n",
    "\n",
    "1. **Helicopter** (H√©licopt√®re) - Sons rotatifs caract√©ristiques\n",
    "2. **Fighter Aircraft** (Avion de chasse) - Bruit de r√©acteurs\n",
    "3. **Military Vehicle** (V√©hicule militaire) - Moteurs lourds\n",
    "4. **Truck** (Camion) - Moteurs diesel\n",
    "5. **Footsteps** (Pas) - Mouvements de personnel\n",
    "6. **Speech** (Parole) - Communications vocales\n",
    "7. **Background** (Fond sonore) - Ambiance\n",
    "\n",
    "### Objectifs du Pr√©traitement\n",
    "\n",
    "‚úÖ Normaliser tous les fichiers audio (16kHz, mono, 10 secondes)  \n",
    "‚úÖ Supprimer les silences et bruits ind√©sirables  \n",
    "‚úÖ Extraire les features acoustiques (mel spectrogrammes, MFCC)  \n",
    "‚úÖ Diviser le dataset de mani√®re stratifi√©e (Train/Val/Test)  \n",
    "‚úÖ Sauvegarder en format HDF5 pour acc√®s rapide  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques n√©cessaires\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import yaml\n",
    "import json\n",
    "import h5py\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration de matplotlib pour de beaux graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration des chemins du projet\n",
    "PROJECT_ROOT = Path(r'c:\\Users\\MDN\\Desktop\\SereneSense')\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw' / 'mad'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed' / 'mad'\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'data' / 'mad_dataset.yaml'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'preprocessing'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es avec succ√®s\")\n",
    "print(f\"üìÅ Dossier projet : {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Donn√©es brutes : {DATA_RAW}\")\n",
    "print(f\"üìÅ Donn√©es trait√©es : {DATA_PROCESSED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. T√©l√©chargement du Dataset MAD {#2-telechargement}\n",
    "\n",
    "### M√©thode de T√©l√©chargement\n",
    "\n",
    "Le dataset MAD a √©t√© t√©l√©charg√© via `kagglehub` en utilisant le script `scripts/download_datasets.py`.\n",
    "\n",
    "### Commande Ex√©cut√©e\n",
    "\n",
    "```bash\n",
    "python scripts/download_datasets.py --datasets mad\n",
    "```\n",
    "\n",
    "### R√©sultats du T√©l√©chargement\n",
    "\n",
    "- **Total d'√©chantillons** : 7,466 fichiers WAV\n",
    "- **Taille compress√©e** : ~2.8 GB\n",
    "- **Destination** : `data/raw/mad/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration officielle du dataset\n",
    "print(\"üìÑ Chargement de la configuration MAD...\\n\")\n",
    "\n",
    "if CONFIG_PATH.exists():\n",
    "    with open(CONFIG_PATH, 'r', encoding='utf-8') as f:\n",
    "        mad_config = yaml.safe_load(f)\n",
    "    \n",
    "    # Extraction des informations cl√©s\n",
    "    dataset_info = mad_config.get('dataset', {})\n",
    "    stats = dataset_info.get('statistics', {})\n",
    "    classes_info = mad_config.get('classes', {})\n",
    "    \n",
    "    print(\"üìä Informations du Dataset MAD :\")\n",
    "    print(f\"  Nom complet        : {dataset_info.get('full_name')}\")\n",
    "    print(f\"  Source             : {dataset_info.get('source')}\")\n",
    "    print(f\"  Total √©chantillons : {stats.get('total_samples')}\")\n",
    "    print(f\"  Nombre de classes  : {stats.get('classes')}\")\n",
    "    print(f\"  Dur√©e totale       : {stats.get('total_duration_hours')} heures\")\n",
    "    print(f\"  Sample rate        : {stats.get('sample_rate')} Hz\")\n",
    "    print(f\"  Profondeur bits    : {stats.get('bit_depth')} bits\")\n",
    "    print(f\"  Taille             : {stats.get('size_gb')} GB\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Fichier de configuration non trouv√© : {CONFIG_PATH}\")\n",
    "    mad_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification de la pr√©sence des donn√©es\n",
    "print(\"\\nüîç V√©rification des donn√©es t√©l√©charg√©es...\\n\")\n",
    "\n",
    "if DATA_RAW.exists():\n",
    "    print(f\"‚úÖ Dossier de donn√©es brutes trouv√© : {DATA_RAW}\")\n",
    "    \n",
    "    # Compter tous les fichiers WAV\n",
    "    audio_files = list(DATA_RAW.rglob('*.wav'))\n",
    "    print(f\"üìä Nombre total de fichiers audio : {len(audio_files)}\")\n",
    "    \n",
    "    if len(audio_files) > 0:\n",
    "        print(f\"\\nüìÑ Exemple de fichiers :\")\n",
    "        for i, file in enumerate(audio_files[:5]):\n",
    "            print(f\"  {i+1}. {file.name}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dossier de donn√©es non trouv√© : {DATA_RAW}\")\n",
    "    print(\"‚ö†Ô∏è Veuillez ex√©cuter : python scripts/download_datasets.py --datasets mad\")\n",
    "    audio_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition des classes\n",
    "CLASS_NAMES = [\n",
    "    'Helicopter',\n",
    "    'Fighter Aircraft', \n",
    "    'Military Vehicle',\n",
    "    'Truck',\n",
    "    'Footsteps',\n",
    "    'Speech',\n",
    "    'Background'\n",
    "]\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Classes du dataset MAD :\\n\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {i}. {class_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Analyse Exploratoire Initiale {#3-analyse-exploratoire}\n",
    "\n",
    "Avant le pr√©traitement, analysons les caract√©ristiques brutes du dataset pour identifier :\n",
    "\n",
    "- Dur√©es variables des fichiers audio\n",
    "- Sample rates diff√©rents\n",
    "- Nombre de canaux (mono/st√©r√©o)\n",
    "- D√©s√©quilibre des classes\n",
    "- Pr√©sence de silences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des caract√©ristiques audio (√©chantillon)\n",
    "print(\"üîç Analyse des caract√©ristiques audio...\\n\")\n",
    "\n",
    "if len(audio_files) > 0:\n",
    "    # √âchantillonner pour acc√©l√©rer l'analyse\n",
    "    sample_size = min(100, len(audio_files))\n",
    "    sampled_files = np.random.choice(audio_files, sample_size, replace=False)\n",
    "    \n",
    "    audio_stats = {\n",
    "        'durations': [],\n",
    "        'sample_rates': [],\n",
    "        'channels': [],\n",
    "        'max_amplitudes': []\n",
    "    }\n",
    "    \n",
    "    for audio_file in tqdm(sampled_files, desc=\"Analyse en cours\"):\n",
    "        try:\n",
    "            # Lire les infos sans charger tout le fichier\n",
    "            info = sf.info(audio_file)\n",
    "            audio_stats['durations'].append(info.duration)\n",
    "            audio_stats['sample_rates'].append(info.samplerate)\n",
    "            audio_stats['channels'].append(info.channels)\n",
    "            \n",
    "            # Charger un court extrait pour l'amplitude\n",
    "            audio, sr = librosa.load(audio_file, sr=None, duration=1.0)\n",
    "            audio_stats['max_amplitudes'].append(np.max(np.abs(audio)))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Cr√©er un DataFrame pour l'analyse\n",
    "    df_stats = pd.DataFrame(audio_stats)\n",
    "    \n",
    "    print(\"\\nüìä Statistiques des fichiers audio :\\n\")\n",
    "    print(df_stats.describe())\n",
    "    \n",
    "    print(f\"\\nüìà D√©tails :\")\n",
    "    print(f\"  Dur√©e moyenne      : {df_stats['durations'].mean():.2f} secondes\")\n",
    "    print(f\"  Dur√©e min/max      : {df_stats['durations'].min():.2f}s / {df_stats['durations'].max():.2f}s\")\n",
    "    print(f\"  Sample rates       : {df_stats['sample_rates'].unique()}\")\n",
    "    print(f\"  Canaux             : {df_stats['channels'].unique()}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è Aucun fichier audio disponible pour l'analyse\")\n",
    "    df_stats = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des caract√©ristiques\n",
    "if df_stats is not None and len(df_stats) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Distribution des dur√©es\n",
    "    axes[0, 0].hist(df_stats['durations'], bins=30, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].axvline(x=10.0, color='red', linestyle='--', linewidth=2, label='Cible: 10s')\n",
    "    axes[0, 0].set_xlabel('Dur√©e (secondes)', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Fr√©quence', fontsize=12)\n",
    "    axes[0, 0].set_title('Distribution des Dur√©es Audio', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Sample rates\n",
    "    sr_counts = df_stats['sample_rates'].value_counts()\n",
    "    axes[0, 1].bar(sr_counts.index.astype(str), sr_counts.values, color='lightcoral', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Sample Rate (Hz)', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Nombre de fichiers', fontsize=12)\n",
    "    axes[0, 1].set_title('Distribution des Sample Rates', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Canaux\n",
    "    channel_counts = df_stats['channels'].value_counts()\n",
    "    axes[1, 0].bar(channel_counts.index.astype(str), channel_counts.values, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_xlabel('Nombre de canaux', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Nombre de fichiers', fontsize=12)\n",
    "    axes[1, 0].set_title('Distribution des Canaux', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Amplitudes\n",
    "    axes[1, 1].hist(df_stats['max_amplitudes'], bins=30, color='plum', edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('Amplitude maximale', fontsize=12)\n",
    "    axes[1, 1].set_ylabel('Fr√©quence', fontsize=12)\n",
    "    axes[1, 1].set_title('Distribution des Amplitudes', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'analyse_exploratoire.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Graphique sauvegard√© : analyse_exploratoire.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Observations Cl√©s\n",
    "\n",
    "Les observations justifient notre pipeline de pr√©traitement :\n",
    "\n",
    "1. **Dur√©es variables** ‚Üí Padding/Trimming √† 10 secondes\n",
    "2. **Sample rates diff√©rents** ‚Üí R√©√©chantillonnage √† 16kHz\n",
    "3. **Mix mono/st√©r√©o** ‚Üí Conversion mono\n",
    "4. **Amplitudes variables** ‚Üí Normalisation Z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Pipeline de Pr√©traitement Audio {#4-pipeline-pretraitement}\n",
    "\n",
    "### √âtapes du Pipeline (script `prepare_data.py`)\n",
    "\n",
    "1. **Chargement** : Lecture du fichier WAV\n",
    "2. **Conversion Mono** : Si st√©r√©o ‚Üí mono\n",
    "3. **R√©√©chantillonnage** : 16,000 Hz\n",
    "4. **Suppression Silence** : Trim (`top_db=30`)\n",
    "5. **Ajustement Dur√©e** : Padding ou Trimming ‚Üí 10s exactement\n",
    "6. **Normalisation** : Z-score (Œº=0, œÉ=1)\n",
    "\n",
    "### Param√®tres Exacts Utilis√©s\n",
    "\n",
    "- **Sample Rate** : 16,000 Hz\n",
    "- **Dur√©e cible** : 10.0 secondes (160,000 samples)\n",
    "- **Seuil silence** : 30 dB\n",
    "- **Range amplitude finale** : [-117.67, 146.06] (valeurs extr√™mes du dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe de pr√©traitement (reproduit la logique de prepare_data.py)\n",
    "class AudioPreprocessor:\n",
    "    \"\"\"Pr√©processeur audio pour le dataset MAD.\"\"\"\n",
    "    \n",
    "    def __init__(self, target_sr=16000, target_duration=10.0, top_db=30):\n",
    "        self.target_sr = target_sr\n",
    "        self.target_duration = target_duration\n",
    "        self.target_length = int(target_sr * target_duration)  # 160,000\n",
    "        self.top_db = top_db\n",
    "    \n",
    "    def load_audio(self, file_path):\n",
    "        \"\"\"Charge et convertit en mono.\"\"\"\n",
    "        audio, sr = librosa.load(file_path, sr=None, mono=True)\n",
    "        return audio, sr\n",
    "    \n",
    "    def resample(self, audio, orig_sr):\n",
    "        \"\"\"R√©√©chantillonne √† 16kHz.\"\"\"\n",
    "        if orig_sr != self.target_sr:\n",
    "            audio = librosa.resample(audio, orig_sr=orig_sr, target_sr=self.target_sr)\n",
    "        return audio\n",
    "    \n",
    "    def remove_silence(self, audio):\n",
    "        \"\"\"Supprime les silences.\"\"\"\n",
    "        audio_trimmed, _ = librosa.effects.trim(audio, top_db=self.top_db)\n",
    "        return audio_trimmed\n",
    "    \n",
    "    def adjust_length(self, audio):\n",
    "        \"\"\"Ajuste √† 10 secondes exactement.\"\"\"\n",
    "        current_length = len(audio)\n",
    "        \n",
    "        if current_length < self.target_length:\n",
    "            # Padding\n",
    "            pad_length = self.target_length - current_length\n",
    "            pad_left = pad_length // 2\n",
    "            pad_right = pad_length - pad_left\n",
    "            audio = np.pad(audio, (pad_left, pad_right), mode='constant')\n",
    "        elif current_length > self.target_length:\n",
    "            # Trimming au centre\n",
    "            start = (current_length - self.target_length) // 2\n",
    "            audio = audio[start:start + self.target_length]\n",
    "        \n",
    "        return audio\n",
    "    \n",
    "    def normalize(self, audio):\n",
    "        \"\"\"Normalisation Z-score.\"\"\"\n",
    "        std = np.std(audio)\n",
    "        if std > 0:\n",
    "            audio = (audio - np.mean(audio)) / std\n",
    "        return audio\n",
    "    \n",
    "    def preprocess(self, file_path):\n",
    "        \"\"\"Pipeline complet.\"\"\"\n",
    "        audio, orig_sr = self.load_audio(file_path)\n",
    "        audio = self.resample(audio, orig_sr)\n",
    "        audio = self.remove_silence(audio)\n",
    "        audio = self.adjust_length(audio)\n",
    "        audio = self.normalize(audio)\n",
    "        return audio\n",
    "\n",
    "# Initialisation\n",
    "preprocessor = AudioPreprocessor(target_sr=16000, target_duration=10.0, top_db=30)\n",
    "\n",
    "print(\"‚úÖ Pr√©processeur audio initialis√©\")\n",
    "print(f\"   - Sample rate : {preprocessor.target_sr} Hz\")\n",
    "print(f\"   - Dur√©e cible : {preprocessor.target_duration}s\")\n",
    "print(f\"   - Samples : {preprocessor.target_length}\")\n",
    "print(f\"   - Seuil silence : {preprocessor.top_db} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de pr√©traitement sur un fichier\n",
    "if len(audio_files) > 0:\n",
    "    example_file = audio_files[0]\n",
    "    print(f\"üìÑ Fichier exemple : {example_file.name}\\n\")\n",
    "    \n",
    "    # Audio original\n",
    "    audio_original, sr_original = librosa.load(example_file, sr=None, mono=True)\n",
    "    \n",
    "    # Audio pr√©trait√©\n",
    "    audio_processed = preprocessor.preprocess(example_file)\n",
    "    \n",
    "    print(f\"üìä Comparaison avant/apr√®s :\")\n",
    "    print(f\"  Original  : {len(audio_original):,} samples @ {sr_original} Hz ({len(audio_original)/sr_original:.2f}s)\")\n",
    "    print(f\"  Pr√©trait√© : {len(audio_processed):,} samples @ {preprocessor.target_sr} Hz ({len(audio_processed)/preprocessor.target_sr:.2f}s)\")\n",
    "    print(f\"  Amplitude : [{audio_processed.min():.2f}, {audio_processed.max():.2f}]\")\n",
    "    print(f\"  Moyenne   : {audio_processed.mean():.6f} (‚âà 0)\")\n",
    "    print(f\"  √âcart-type: {audio_processed.std():.6f} (‚âà 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du pipeline de pr√©traitement\n",
    "if len(audio_files) > 0:\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Original\n",
    "    time_orig = np.arange(len(audio_original)) / sr_original\n",
    "    axes[0].plot(time_orig, audio_original, color='steelblue', linewidth=0.5)\n",
    "    axes[0].set_title('1. Audio Original (Brut)', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_xlabel('Temps (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Apr√®s suppression silence\n",
    "    audio_no_silence = preprocessor.remove_silence(\n",
    "        preprocessor.resample(audio_original, sr_original)\n",
    "    )\n",
    "    time_no_sil = np.arange(len(audio_no_silence)) / preprocessor.target_sr\n",
    "    axes[1].plot(time_no_sil, audio_no_silence, color='darkorange', linewidth=0.5)\n",
    "    axes[1].set_title('2. Apr√®s Suppression Silence', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_xlabel('Temps (s)')\n",
    "    axes[1].set_ylabel('Amplitude')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Apr√®s ajustement longueur\n",
    "    audio_adjusted = preprocessor.adjust_length(audio_no_silence)\n",
    "    time_adj = np.arange(len(audio_adjusted)) / preprocessor.target_sr\n",
    "    axes[2].plot(time_adj, audio_adjusted, color='forestgreen', linewidth=0.5)\n",
    "    axes[2].axvline(x=10.0, color='red', linestyle='--', linewidth=1.5, label='10s')\n",
    "    axes[2].set_title('3. Apr√®s Ajustement √† 10 secondes', fontsize=13, fontweight='bold')\n",
    "    axes[2].set_xlabel('Temps (s)')\n",
    "    axes[2].set_ylabel('Amplitude')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].set_xlim([0, 10])\n",
    "    \n",
    "    # 4. Final (normalis√©)\n",
    "    time_proc = np.arange(len(audio_processed)) / preprocessor.target_sr\n",
    "    axes[3].plot(time_proc, audio_processed, color='purple', linewidth=0.5)\n",
    "    axes[3].axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "    axes[3].axhline(y=1, color='red', linestyle='--', linewidth=1, alpha=0.5, label='¬±1œÉ')\n",
    "    axes[3].axhline(y=-1, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "    axes[3].set_title('4. Final : Normalis√© Z-score', fontsize=13, fontweight='bold')\n",
    "    axes[3].set_xlabel('Temps (s)')\n",
    "    axes[3].set_ylabel('Amplitude Normalis√©e')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    axes[3].set_xlim([0, 10])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'preprocessing_pipeline.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Graphique sauvegard√© : preprocessing_pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ R√©sultats du Pr√©traitement\n",
    "\n",
    "**Transformations r√©ussies** :\n",
    "1. Audio original ‚Üí dur√©e et SR variables\n",
    "2. Silences retir√©s ‚Üí contenu utile pr√©serv√©\n",
    "3. Ajust√© √† 10s ‚Üí exactement 160,000 samples\n",
    "4. Normalis√© ‚Üí Œº‚âà0, œÉ‚âà1\n",
    "\n",
    "**Range d'amplitudes sur tout le dataset** : [-117.67, 146.06]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Extraction des Features {#5-extraction-features}\n",
    "\n",
    "### Types de Features Extraites\n",
    "\n",
    "#### 1. Mel Spectrogramme (AudioMAE)\n",
    "- **n_fft** : 1024\n",
    "- **hop_length** : 160 (10ms)\n",
    "- **n_mels** : 128\n",
    "- **f_min / f_max** : 50 Hz / 8000 Hz\n",
    "- **Dimension** : (128, 1000) ‚Üí resize (128, 128)\n",
    "\n",
    "#### 2. MFCC (CNN et CRNN)\n",
    "- **n_mfcc** : 40 coefficients\n",
    "- **n_mels** : 64\n",
    "- **n_fft** : 1024\n",
    "- **hop_length** : 512 (31.25ms)\n",
    "- **Canaux** : 3 (MFCC + Œî + Œî¬≤)\n",
    "- **Dimension CNN** : (3, 40, 92) pour 3s\n",
    "- **Dimension CRNN** : (3, 40, 124) pour 4s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe d'extraction de features\n",
    "class FeatureExtractor:\n",
    "    \"\"\"Extracteur de features audio.\"\"\"\n",
    "    \n",
    "    def __init__(self, sr=16000):\n",
    "        self.sr = sr\n",
    "    \n",
    "    def extract_mel_spectrogram(self, audio, n_mels=128, n_fft=1024, \n",
    "                               hop_length=160, f_min=50, f_max=8000):\n",
    "        \"\"\"Mel spectrogramme pour AudioMAE.\"\"\"\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, sr=self.sr, n_fft=n_fft, hop_length=hop_length,\n",
    "            n_mels=n_mels, fmin=f_min, fmax=f_max\n",
    "        )\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec_db\n",
    "    \n",
    "    def extract_mfcc(self, audio, n_mfcc=40, n_mels=64, n_fft=1024, hop_length=512):\n",
    "        \"\"\"MFCC avec deltas pour CNN/CRNN.\"\"\"\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=audio, sr=self.sr, n_mfcc=n_mfcc, n_mels=n_mels,\n",
    "            n_fft=n_fft, hop_length=hop_length\n",
    "        )\n",
    "        mfcc_delta = librosa.feature.delta(mfcc)\n",
    "        mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        mfcc_features = np.stack([mfcc, mfcc_delta, mfcc_delta2], axis=0)\n",
    "        return mfcc_features\n",
    "\n",
    "feature_extractor = FeatureExtractor(sr=16000)\n",
    "print(\"‚úÖ Extracteur de features initialis√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des features sur l'exemple\n",
    "if len(audio_files) > 0:\n",
    "    # Mel spectrogramme\n",
    "    mel_spec = feature_extractor.extract_mel_spectrogram(audio_processed)\n",
    "    \n",
    "    # MFCC pour CNN (3s)\n",
    "    audio_3s = audio_processed[:3 * 16000]\n",
    "    mfcc_cnn = feature_extractor.extract_mfcc(audio_3s)\n",
    "    \n",
    "    # MFCC pour CRNN (4s)\n",
    "    audio_4s = audio_processed[:4 * 16000]\n",
    "    mfcc_crnn = feature_extractor.extract_mfcc(audio_4s)\n",
    "    \n",
    "    print(\"üìä Dimensions des features extraites :\")\n",
    "    print(f\"  Mel Spectrogramme : {mel_spec.shape}\")\n",
    "    print(f\"  MFCC CNN (3s)     : {mfcc_cnn.shape}\")\n",
    "    print(f\"  MFCC CRNN (4s)    : {mfcc_crnn.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des features\n",
    "if len(audio_files) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    \n",
    "    # 1. Mel Spectrogramme\n",
    "    img1 = librosa.display.specshow(\n",
    "        mel_spec, sr=16000, hop_length=160, x_axis='time', y_axis='mel',\n",
    "        fmin=50, fmax=8000, ax=axes[0, 0], cmap='viridis'\n",
    "    )\n",
    "    axes[0, 0].set_title('Mel Spectrogramme (AudioMAE)\\n128 mels, 50-8000 Hz', \n",
    "                         fontsize=13, fontweight='bold')\n",
    "    fig.colorbar(img1, ax=axes[0, 0], format='%+2.0f dB')\n",
    "    \n",
    "    # 2. MFCC (canal 1)\n",
    "    img2 = librosa.display.specshow(\n",
    "        mfcc_cnn[0], sr=16000, hop_length=512, x_axis='time',\n",
    "        ax=axes[0, 1], cmap='coolwarm'\n",
    "    )\n",
    "    axes[0, 1].set_title('MFCC CNN (3s) - Canal 1/3', fontsize=13, fontweight='bold')\n",
    "    axes[0, 1].set_ylabel('Coefficient MFCC')\n",
    "    fig.colorbar(img2, ax=axes[0, 1])\n",
    "    \n",
    "    # 3. Delta\n",
    "    img3 = librosa.display.specshow(\n",
    "        mfcc_cnn[1], sr=16000, hop_length=512, x_axis='time',\n",
    "        ax=axes[1, 0], cmap='coolwarm'\n",
    "    )\n",
    "    axes[1, 0].set_title('MFCC Delta (Canal 2/3)', fontsize=13, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Coefficient MFCC')\n",
    "    fig.colorbar(img3, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Delta-Delta\n",
    "    img4 = librosa.display.specshow(\n",
    "        mfcc_cnn[2], sr=16000, hop_length=512, x_axis='time',\n",
    "        ax=axes[1, 1], cmap='coolwarm'\n",
    "    )\n",
    "    axes[1, 1].set_title('MFCC Delta-Delta (Canal 3/3)', fontsize=13, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Coefficient MFCC')\n",
    "    fig.colorbar(img4, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'feature_extraction.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Graphique sauvegard√© : feature_extraction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Interpr√©tation des Features\n",
    "\n",
    "**Mel Spectrogramme** :\n",
    "- Repr√©sentation temps-fr√©quence optimis√©e\n",
    "- Signatures caract√©ristiques des v√©hicules militaires\n",
    "- Utilis√© par AudioMAE (Vision Transformer)\n",
    "\n",
    "**MFCC** :\n",
    "- Capture l'enveloppe spectrale\n",
    "- Deltas = variations temporelles\n",
    "- Delta-deltas = acc√©l√©ration\n",
    "- Utilis√© par CNN et CRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Division Stratifi√©e du Dataset {#6-division-dataset}\n",
    "\n",
    "### Strat√©gie de Division\n",
    "\n",
    "- **Train** : 70% (5,226 √©chantillons)\n",
    "- **Validation** : 15% (1,120 √©chantillons)\n",
    "- **Test** : 15% (1,120 √©chantillons)\n",
    "- **Total** : 7,466 √©chantillons\n",
    "\n",
    "### Division Stratifi√©e\n",
    "Chaque ensemble maintient les m√™mes proportions de classes.\n",
    "\n",
    "### Reproductibilit√©\n",
    "Seed al√©atoire : **42**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres de division\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO = 0.15\n",
    "TEST_RATIO = 0.15\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "TRAIN_SAMPLES = 5226\n",
    "VAL_SAMPLES = 1120\n",
    "TEST_SAMPLES = 1120\n",
    "TOTAL_SAMPLES = 7466\n",
    "\n",
    "print(\"üìä Configuration de la division :\")\n",
    "print(f\"  Train      : {TRAIN_RATIO*100:.0f}% ({TRAIN_SAMPLES:,} √©chantillons)\")\n",
    "print(f\"  Validation : {VAL_RATIO*100:.0f}% ({VAL_SAMPLES:,} √©chantillons)\")\n",
    "print(f\"  Test       : {TEST_RATIO*100:.0f}% ({TEST_SAMPLES:,} √©chantillons)\")\n",
    "print(f\"  Total      : {TOTAL_SAMPLES:,} √©chantillons\")\n",
    "print(f\"  Seed       : {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des fichiers HDF5 g√©n√©r√©s\n",
    "print(\"\\nüîç V√©rification des fichiers pr√©trait√©s...\\n\")\n",
    "\n",
    "if DATA_PROCESSED.exists():\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        split_dir = DATA_PROCESSED / split\n",
    "        h5_file = split_dir / f\"{split}.h5\"\n",
    "        meta_file = split_dir / \"metadata.json\"\n",
    "        \n",
    "        print(f\"üìÅ Split '{split}' :\")\n",
    "        \n",
    "        if h5_file.exists():\n",
    "            print(f\"  ‚úÖ HDF5 : {h5_file}\")\n",
    "            \n",
    "            # Lire les infos du fichier HDF5\n",
    "            try:\n",
    "                with h5py.File(h5_file, 'r') as f:\n",
    "                    if 'audio' in f:\n",
    "                        print(f\"     Shape : {f['audio'].shape}\")\n",
    "                        print(f\"     Dtype : {f['audio'].dtype}\")\n",
    "            except:\n",
    "                pass\n",
    "        else:\n",
    "            print(f\"  ‚ùå HDF5 manquant\")\n",
    "        \n",
    "        if meta_file.exists():\n",
    "            print(f\"  ‚úÖ M√©tadonn√©es : {meta_file}\")\n",
    "            with open(meta_file, 'r') as f:\n",
    "                meta = json.load(f)\n",
    "                n_samples = meta.get('num_samples', meta.get('num_items', 0))\n",
    "                print(f\"     √âchantillons : {n_samples}\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå M√©tadonn√©es manquantes\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dossier de donn√©es pr√©trait√©es non trouv√©\")\n",
    "    print(\"   Ex√©cutez : python scripts/prepare_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Graphique 1 : R√©partition par split\n",
    "splits = ['Train', 'Validation', 'Test']\n",
    "counts = [TRAIN_SAMPLES, VAL_SAMPLES, TEST_SAMPLES]\n",
    "colors_split = ['steelblue', 'orange', 'forestgreen']\n",
    "\n",
    "axes[0].bar(splits, counts, color=colors_split, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Nombre d\\'√©chantillons', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('R√©partition Train/Val/Test', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for i, (split, count) in enumerate(zip(splits, counts)):\n",
    "    axes[0].text(i, count + 100, f\"{count:,}\\n({count/TOTAL_SAMPLES*100:.0f}%)\", \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Graphique 2 : Pie chart\n",
    "axes[1].pie(counts, labels=splits, colors=colors_split, autopct='%1.0f%%',\n",
    "           startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
    "axes[1].set_title(f'Distribution du Dataset\\n(Total: {TOTAL_SAMPLES:,} √©chantillons)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'dataset_split.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Graphique sauvegard√© : dataset_split.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Format HDF5\n",
    "\n",
    "Structure des donn√©es pr√©trait√©es :\n",
    "\n",
    "```\n",
    "data/processed/mad/\n",
    "‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train.h5 (5,226 √©chantillons)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ manifest_train.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metadata.json\n",
    "‚îú‚îÄ‚îÄ validation/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validation.h5 (1,120 √©chantillons)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ manifest_validation.json\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metadata.json\n",
    "‚îî‚îÄ‚îÄ test/\n",
    "    ‚îú‚îÄ‚îÄ test.h5 (1,120 √©chantillons)\n",
    "    ‚îú‚îÄ‚îÄ manifest_test.json\n",
    "    ‚îî‚îÄ‚îÄ metadata.json\n",
    "```\n",
    "\n",
    "**Avantages HDF5** :\n",
    "- ‚úÖ Acc√®s rapide\n",
    "- ‚úÖ Compression ~50%\n",
    "- ‚úÖ Compatible PyTorch\n",
    "- ‚úÖ Ne charge pas tout en m√©moire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Visualisations Finales {#7-visualisations-finales}\n",
    "\n",
    "Exemples d'√©chantillons pr√©trait√©s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation d'exemples multiples\n",
    "if len(audio_files) > 0:\n",
    "    n_examples = min(3, len(audio_files))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_examples, 2, figsize=(16, 4 * n_examples))\n",
    "    if n_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i in range(n_examples):\n",
    "        file = audio_files[i]\n",
    "        audio_proc = preprocessor.preprocess(file)\n",
    "        mel_spec = feature_extractor.extract_mel_spectrogram(audio_proc)\n",
    "        \n",
    "        # Waveform\n",
    "        time = np.arange(len(audio_proc)) / 16000\n",
    "        axes[i, 0].plot(time, audio_proc, linewidth=0.5, color='steelblue')\n",
    "        axes[i, 0].set_title(f'Exemple {i+1} - Waveform\\n{file.name}', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        axes[i, 0].set_xlabel('Temps (s)')\n",
    "        axes[i, 0].set_ylabel('Amplitude')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        axes[i, 0].set_xlim([0, 10])\n",
    "        \n",
    "        # Mel Spectrogramme\n",
    "        img = librosa.display.specshow(\n",
    "            mel_spec, sr=16000, hop_length=160, x_axis='time', y_axis='mel',\n",
    "            fmin=50, fmax=8000, ax=axes[i, 1], cmap='viridis'\n",
    "        )\n",
    "        axes[i, 1].set_title(f'Exemple {i+1} - Mel Spectrogramme', \n",
    "                            fontsize=12, fontweight='bold')\n",
    "        fig.colorbar(img, ax=axes[i, 1], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTPUT_DIR / 'examples_preprocessed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Graphique sauvegard√© : examples_preprocessed.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© final du pr√©traitement\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \"*25 + \"üìä R√âSUM√â DU PR√âTRAITEMENT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ DATASET MAD PR√âTRAIT√â\")\n",
    "print(f\"   ‚Ä¢ Total : 7,466 √©chantillons\")\n",
    "print(f\"   ‚Ä¢ Classes : 7 (v√©hicules militaires)\")\n",
    "print(f\"   ‚Ä¢ Format : HDF5\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ CARACT√âRISTIQUES AUDIO\")\n",
    "print(f\"   ‚Ä¢ Sample rate : 16,000 Hz\")\n",
    "print(f\"   ‚Ä¢ Dur√©e : 10.0 secondes\")\n",
    "print(f\"   ‚Ä¢ Samples : 160,000\")\n",
    "print(f\"   ‚Ä¢ Canaux : Mono (1)\")\n",
    "print(f\"   ‚Ä¢ Normalisation : Z-score (Œº=0, œÉ=1)\")\n",
    "print(f\"   ‚Ä¢ Amplitude range : [-117.67, 146.06]\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ DIVISION DU DATASET\")\n",
    "print(f\"   ‚Ä¢ Train : 5,226 (70%)\")\n",
    "print(f\"   ‚Ä¢ Validation : 1,120 (15%)\")\n",
    "print(f\"   ‚Ä¢ Test : 1,120 (15%)\")\n",
    "print(f\"   ‚Ä¢ Stratifi√©e : Oui\")\n",
    "print(f\"   ‚Ä¢ Seed : 42\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ FEATURES EXTRAITES\")\n",
    "print(f\"   ‚Ä¢ Mel Spectrogramme : (128, 1000) ‚Üí (128, 128)\")\n",
    "print(f\"     - Pour AudioMAE\")\n",
    "print(f\"   ‚Ä¢ MFCC CNN : (3, 40, 92) - 3s\")\n",
    "print(f\"   ‚Ä¢ MFCC CRNN : (3, 40, 124) - 4s\")\n",
    "print()\n",
    "\n",
    "print(\"‚úÖ PIPELINE APPLIQU√â\")\n",
    "print(f\"   1. Chargement + conversion mono\")\n",
    "print(f\"   2. R√©√©chantillonnage 16kHz\")\n",
    "print(f\"   3. Suppression silences (30dB)\")\n",
    "print(f\"   4. Padding/Trimming √† 10s\")\n",
    "print(f\"   5. Normalisation Z-score\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\" \"*20 + \"üéØ Pr√©traitement termin√© avec succ√®s!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Conclusion {#8-conclusion}\n",
    "\n",
    "### R√©capitulatif\n",
    "\n",
    "Ce notebook a document√© le pr√©traitement complet du dataset MAD :\n",
    "\n",
    "1. ‚úÖ **T√©l√©chargement** : 7,466 √©chantillons, 7 classes\n",
    "2. ‚úÖ **Analyse exploratoire** : Identification des probl√®mes\n",
    "3. ‚úÖ **Pipeline de pr√©traitement** : 5 √©tapes reproductibles\n",
    "4. ‚úÖ **Extraction de features** : Mel Spectrogram + MFCC\n",
    "5. ‚úÖ **Division stratifi√©e** : Train 70% / Val 15% / Test 15%\n",
    "6. ‚úÖ **Sauvegarde HDF5** : Acc√®s rapide pour l'entra√Ænement\n",
    "\n",
    "### R√©sultats Cl√©s\n",
    "\n",
    "- üìä 7,466 √©chantillons normalis√©s\n",
    "- üéµ 10 secondes (160,000 samples @ 16kHz)\n",
    "- üìà Normalisation Z-score\n",
    "- üîä Features optimis√©es\n",
    "- üíæ Format HDF5\n",
    "\n",
    "### Prochaines √âtapes\n",
    "\n",
    "1. **Notebook 2** : CNN-MFCC (66.88% accuracy)\n",
    "2. **Notebook 3** : CRNN-MFCC (73.21% accuracy)\n",
    "3. **Notebook 4** : AudioMAE (82.15% accuracy)\n",
    "4. **Notebook 5** : D√©ploiement Raspberry Pi 5\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px; background-color: #e8f4f8; border-radius: 10px;\">\n",
    "    <h3>üéâ Notebook 1 Compl√©t√© !</h3>\n",
    "    <p><b>Projet SereneSense - D√©tection de Sons Militaires</b></p>\n",
    "    <p>Donn√©es pr√©trait√©es et pr√™tes pour l'entra√Ænement</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
