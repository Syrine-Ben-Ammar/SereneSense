{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SereneSense Data Exploration & Analysis\n\nExplore and understand the MAD dataset structure and characteristics.\n\n**Duration**: ~15 minutes\n**Topics**: Dataset loading, EDA, audio statistics, spectrograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport librosa\nfrom pathlib import Path\nfrom collections import Counter\n\n# Load MAD dataset config\nwith open('../configs/data/mad_dataset.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nprint('\ud83d\udcca MAD Dataset Configuration:')\nprint(f\"  Name: {config['dataset']['name']}\")\nprint(f\"  Total samples: {config['dataset']['statistics']['total_samples']}\")\nprint(f\"  Classes: {config['dataset']['statistics']['classes']}\")\nprint(f\"  Total duration: {config['dataset']['statistics']['total_duration_hours']}h\")\nprint(f\"  Sample rate: {config['dataset']['statistics']['sample_rate']} Hz\")\nprint(f\"  License: {config['dataset']['license']['type']}\")\nprint(f\"  Download: {config['source']['download']['url']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Class Distribution\n\nVisualize the distribution of samples across classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution from config\nclass_dist = config['subsets']['train']['class_distribution']\nclasses = list(class_dist.keys())\nsamples = list(class_dist.values())\n\n# Create visualization\nfig = go.Figure(data=[\n    go.Bar(\n        x=classes,\n        y=samples,\n        marker=dict(color='lightblue', line=dict(color='darkblue', width=1.5))\n    )\n])\n\nfig.update_layout(\n    title='MAD Dataset: Class Distribution',\n    xaxis_title='Sound Class',\n    yaxis_title='Number of Samples',\n    height=400,\n    template='plotly_white'\n)\nfig.show()\n\nprint(f'\u2713 Total samples: {sum(samples)}')\nprint(f'\u2713 Average samples per class: {np.mean(samples):.0f}')\nprint(f'\u2713 Std deviation: {np.std(samples):.0f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio Feature Statistics\n\nAnalyze audio characteristics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Audio statistics\nsr = config['dataset']['statistics']['sample_rate']\nprint(f'\ud83d\udcc8 Audio Statistics:')\nprint(f'  Sample rate: {sr:,} Hz')\nprint(f'  Bit depth: {config[\"dataset\"][\"statistics\"][\"bit_depth\"]} bits')\nprint(f'  Duration per sample: {config[\"processing\"][\"duration_seconds\"]} seconds')\nprint(f'  Target sample rate: {config[\"processing\"][\"target_sample_rate\"]} Hz')\nprint(f'  Target channels: {config[\"processing\"][\"target_channels\"]}')\nprint(f'  Expected samples per file: {sr * config[\"processing\"][\"duration_seconds\"]:,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic Data Exploration\n\nSince we're in a notebook environment, let's analyze synthetic examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic samples for each class\nsr = 16000\nduration = 5\nt = np.linspace(0, duration, int(sr * duration))\n\n# Different frequency characteristics for each class\nclass_characteristics = {\n    'Helicopter': {'freqs': [100, 300, 600], 'name': 'Low-frequency rotations'},\n    'Fighter Aircraft': {'freqs': [500, 1500, 3000], 'name': 'High-frequency turbines'},\n    'Military Vehicle': {'freqs': [150, 400, 900], 'name': 'Engine rumble'},\n    'Truck': {'freqs': [100, 250, 500], 'name': 'Heavy engine'},\n    'Footsteps': {'freqs': [100, 200], 'name': 'Rhythmic impacts'},\n    'Speech': {'freqs': [300, 500, 2000], 'name': 'Vocal frequencies'},\n    'Background': {'freqs': [50, 100, 200], 'name': 'Ambient noise'}\n}\n\nfig, axes = plt.subplots(4, 2, figsize=(14, 12))\nfig.suptitle('Audio Characteristics by Class', fontsize=14, fontweight='bold')\n\nfor idx, (class_name, info) in enumerate(list(class_characteristics.items())[:7]):\n    row, col = idx // 2, idx % 2\n    ax = axes[row, col]\n    \n    # Create synthetic audio\n    audio = np.zeros_like(t)\n    for freq in info['freqs']:\n        audio += 0.3 * np.sin(2 * np.pi * freq * t)\n    audio += 0.05 * np.random.randn(len(audio))\n    audio = audio / np.max(np.abs(audio)) * 0.9\n    \n    # Plot spectrogram\n    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=32, fmax=4000)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    \n    im = ax.imshow(mel_spec_db, aspect='auto', origin='lower', cmap='viridis')\n    ax.set_title(f'{class_name}\\n({info[\"name\"]})', fontweight='bold')\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Freq (Hz)')\n    plt.colorbar(im, ax=ax, label='dB')\n\naxes[3, 1].axis('off')\nplt.tight_layout()\nplt.show()\n\nprint('\u2713 Spectrogram analysis complete')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splits\n\nUnderstand the train/val/test splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "splits_info = config['subsets']\n\ntotal_samples = sum(splits_info['train']['class_distribution'].values())\ntrain_samples = sum(splits_info['train']['class_distribution'].values())\nval_samples = sum(splits_info['validation']['class_distribution'].values())\ntest_samples = sum(splits_info['test']['class_distribution'].values())\n\nprint('\ud83d\udccb Data Splits:')\nprint(f'  Training: {train_samples:,} samples ({train_samples/total_samples*100:.1f}%)')\nprint(f'  Validation: {val_samples:,} samples ({val_samples/total_samples*100:.1f}%)')\nprint(f'  Test: {test_samples:,} samples ({test_samples/total_samples*100:.1f}%)')\nprint(f'  Total: {total_samples:,} samples')\n\n# Visualize splits\nfig = go.Figure(data=[go.Pie(\n    labels=['Train', 'Validation', 'Test'],\n    values=[train_samples, val_samples, test_samples],\n    hole=0.3,\n    marker=dict(colors=['lightgreen', 'lightyellow', 'lightcoral'])\n)])\nfig.update_layout(title='Data Split Distribution', height=400)\nfig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n\u2713 MAD dataset contains 8,075 samples across 7 military-relevant sound classes\n\u2713 Balanced class distribution ensures fair model training\n\u2713 16kHz sample rate is standard for audio classification\n\u2713 10-second clips allow temporal pattern learning\n\u2713 Multiple domain sources provide diversity\n\nNext: See `03_model_training.ipynb` to train on this data!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}