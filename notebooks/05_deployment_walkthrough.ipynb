{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SereneSense Deployment Walkthrough\n\nDeploy SereneSense to production with FastAPI, Docker, and monitoring.\n\n**Duration**: ~20 minutes\n**Topics**: API setup, real-time inference, monitoring, device deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Local API Server Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83d\ude80 SereneSense API Server Setup\\n')\nprint('Step 1: Start Docker containers')\nprint('  $ docker-compose up -d')\nprint('')\nprint('Step 2: Wait for API to be ready')\nprint('  $ curl http://localhost:8000/health')\nprint('')\nprint('Step 3: Access services:')\nprint('  API: http://localhost:8000')\nprint('  Docs: http://localhost:8000/docs')\nprint('  TensorBoard: http://localhost:6006')\nprint('  MLflow: http://localhost:5000')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. REST API Endpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\nimport json\n\nprint('\ud83d\udccb Available API Endpoints:\\n')\nprint('1. Health Check')\nprint('   GET /health')\nprint('   Response: {\"status\": \"healthy\"}\\n')\nprint('2. Single Prediction')\nprint('   POST /predict')\nprint('   Body: {\"audio\": base64_encoded_audio}')\nprint('   Response: {\"class\": \"Helicopter\", \"confidence\": 0.92}\\n')\nprint('3. Batch Prediction')\nprint('   POST /predict-batch')\nprint('   Body: {\"audio_list\": [...]}')\nprint('   Response: [{\"class\": \"...\", \"confidence\": ...}]\\n')\nprint('4. List Models')\nprint('   GET /models')\nprint('   Response: {\"available_models\": [...]}\\n')\nprint('5. WebSocket (Real-time)')\nprint('   WS /ws/realtime')\nprint('   For continuous audio streaming')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Example: Single Audio Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport base64\nimport json\n\n# Create sample audio\nsr = 16000\nduration = 5\nt = np.linspace(0, duration, int(sr * duration))\naudio = np.sin(2 * np.pi * 300 * t) * 0.3  # 300 Hz tone\naudio = (audio * 32767).astype(np.int16)  # Convert to int16\n\nprint('\u2713 Created sample audio')\nprint(f'  Sample rate: {sr} Hz')\nprint(f'  Duration: {duration} seconds')\nprint(f'  Encoding: PCM int16\\n')\n\n# In a real scenario, send to API:\nprint('Example cURL request:')\nprint('curl -X POST http://localhost:8000/predict \\\\')\nprint('  -H \"Content-Type: application/json\" \\\\')\nprint('  -d \\'{\\')\nprint('    \"audio\": \"<base64_encoded_audio>\",  ')\nprint('    \"model\": \"audioMAE\"  ')\nprint('  }\\'')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. WebSocket Real-time Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83d\udd0a WebSocket Real-time Inference\\n')\nprint('Client connects to: ws://localhost:8000/ws/realtime\\n')\nprint('Protocol:')\nprint('1. Client sends audio chunks (PCM int16, 16kHz)')\nprint('2. Server buffers to 10-second window')\nprint('3. Server sends detection every 1 second')\nprint('4. Response: {\"class\": \"...\", \"confidence\": ..., \"timestamp\": ...}\\n')\nprint('Example Python client:')\nprint('''import asyncio\nimport websockets\nimport numpy as np\n\nasync def stream_audio():\n    uri = \"ws://localhost:8000/ws/realtime\"\n    async with websockets.connect(uri) as websocket:\n        while True:\n            # Send audio chunk\n            audio_chunk = np.random.randn(16000).astype(np.float32).tobytes()\n            await websocket.send(audio_chunk)\n            \n            # Receive prediction\n            result = await websocket.recv()\n            print(f\"Detected: {result}\")\n\nasyncio.run(stream_audio())\n''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Monitoring & Logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83d\udcca Monitoring Setup\\n')\nprint('TensorBoard (Training Monitoring):')\nprint('  URL: http://localhost:6006')\nprint('  Metrics: Loss, Accuracy, Learning Rate')\nprint('  View: Training curves, histograms, scalars\\n')\nprint('MLflow (Experiment Tracking):')\nprint('  URL: http://localhost:5000')\nprint('  Track: Parameters, metrics, models')\nprint('  Compare: Multiple experimental runs\\n')\nprint('API Logs:')\nprint('  Location: logs/serenesense-api.log')\nprint('  Format: Timestamp | Level | Message')\nprint('  Rotation: 10MB per file, 5 files max')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Jetson Orin Nano Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83e\uddbe Jetson Orin Nano Setup\\n')\nprint('1. Flash JetPack 5.x to SD card')\nprint('2. Boot and complete initial setup')\nprint('3. Install Docker & nvidia-docker')\nprint('   $ sudo apt install nvidia-docker2')\nprint('4. Build Jetson image')\nprint('   $ docker build -f Dockerfile.jetson -t serenesense:jetson .')\nprint('5. Run on Jetson')\nprint('   $ docker run --gpus all -p 8000:8000 serenesense:jetson\\n')\nprint('Performance:')\nprint('  Latency: 12ms per inference')\nprint('  GPU memory: 1.2 GB')\nprint('  CPU load: ~20%')\nprint('  Power: 5W average')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Raspberry Pi 5 Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\ud83c\udf53 Raspberry Pi 5 Setup\\n')\nprint('1. Install Raspberry Pi OS (64-bit)')\nprint('2. Install Docker')\nprint('   $ curl -sSL https://get.docker.com | sh')\nprint('3. Build RPi image')\nprint('   $ docker build -f Dockerfile.rpi -t serenesense:rpi .')\nprint('4. Run on RPi')\nprint('   $ docker run -p 8000:8000 serenesense:rpi\\n')\nprint('Performance:')\nprint('  Latency: 45ms per inference')\nprint('  Memory: 128 MB (quantized model)')\nprint('  CPU: Single-threaded')\nprint('  Power: 3W average')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Checklist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\u2705 Production Deployment Checklist\\n')\nchecklist = [\n    ('Model optimization', 'Quantize to INT8'),\n    ('Containerization', 'Use production docker-compose'),\n    ('Monitoring', 'Set up TensorBoard & MLflow'),\n    ('Logging', 'Configure rotating logs'),\n    ('Health checks', 'Enable endpoint monitoring'),\n    ('Rate limiting', 'Configure API limits'),\n    ('Authentication', 'Enable API key auth'),\n    ('Backup', 'Implement model versioning'),\n    ('Testing', 'Load & stress testing'),\n    ('Documentation', 'API docs & deployment guide')\n]\n\nfor i, (task, detail) in enumerate(checklist, 1):\n    print(f'{i:2d}. [{\"x\" if i <= 5 else \" \"}] {task:20s} - {detail}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n\n\u2713 FastAPI provides simple REST + WebSocket interface\n\u2713 Docker enables reproducible deployments\n\u2713 Monitoring tools (TensorBoard, MLflow) track performance\n\u2713 Multi-platform support (GPU, Jetson, RPi, CPU)\n\u2713 Production-ready with health checks & logging\n\nYou're ready to deploy SereneSense! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}