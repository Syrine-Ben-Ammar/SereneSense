{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4 : Entra√Ænement du Mod√®le AudioMAE\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table des Mati√®res\n",
    "\n",
    "1. [Introduction et Contexte](#1-introduction)\n",
    "2. [Architecture Vision Transformer AudioMAE](#2-architecture)\n",
    "3. [Comparaison avec CNN et CRNN](#3-comparaison)\n",
    "4. [Configuration d'Entra√Ænement](#4-configuration)\n",
    "5. [Processus d'Entra√Ænement](#5-entrainement)\n",
    "6. [R√©sultats Exceptionnels](#6-resultats)\n",
    "7. [Analyse de la G√©n√©ralisation](#7-generalisation)\n",
    "8. [Visualisations Compl√®tes](#8-visualisations)\n",
    "9. [Conclusion et Impact](#9-conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction et Contexte {#1-introduction}\n",
    "\n",
    "### Objectif\n",
    "\n",
    "Ce notebook documente l'entra√Ænement du mod√®le **AudioMAE** (Audio Masked Autoencoder), le mod√®le le plus performant du projet SereneSense, bas√© sur l'architecture **Vision Transformer**.\n",
    "\n",
    "### √âvolution des Mod√®les\n",
    "\n",
    "**Progression historique** :\n",
    "1. **CNN-MFCC (Baseline)** : 66.88% accuracy, 242K params\n",
    "2. **CRNN-MFCC** : 73.21% accuracy (+6.3%), 1.5M params\n",
    "3. **AudioMAE** : **82.15% accuracy (+8.94%)**, 111M params\n",
    "\n",
    "### Pourquoi AudioMAE ?\n",
    "\n",
    "Les mod√®les MFCC avaient des limites :\n",
    "- ‚ùå Features MFCC compress√©es (perte d'information)\n",
    "- ‚ùå Dur√©e audio courte (3-4 secondes)\n",
    "- ‚ùå Architecture limit√©e (CNN/RNN classiques)\n",
    "- ‚ùå Overfitting (CNN) ou capacit√© limit√©e\n",
    "\n",
    "AudioMAE apporte une r√©volution :\n",
    "- ‚úÖ **Vision Transformer** : Architecture state-of-the-art\n",
    "- ‚úÖ **Mel Spectrogramme** : Repr√©sentation compl√®te 128√ó128\n",
    "- ‚úÖ **10 secondes d'audio** : Contexte 2.5√ó plus long\n",
    "- ‚úÖ **111M param√®tres** : Capacit√© massive\n",
    "- ‚úÖ **G√©n√©ralisation exceptionnelle** : Val Acc > Train Acc!\n",
    "\n",
    "### R√©sultats Exceptionnels\n",
    "\n",
    "**M√©triques finales (Epoch 100)** :\n",
    "- **Validation Accuracy** : **82.15%** üéØ\n",
    "- **Training Accuracy** : 69.77%\n",
    "- **G√©n√©ralisation** : +12.38% (val > train!)\n",
    "- **Validation Loss** : 0.8693\n",
    "- **Training Loss** : 0.9763\n",
    "- **Training Time** : 237.7 minutes (~4 heures)\n",
    "- **Performance Grade** : **A- / B+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des biblioth√®ques\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Chemins\n",
    "PROJECT_ROOT = Path(r'c:\\Users\\MDN\\Desktop\\SereneSense')\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 'models' / 'audioMAE.yaml'\n",
    "TRAINING_CONFIG = PROJECT_ROOT / 'outputs' / 'training_config.json'\n",
    "RESULTS_PATH = PROJECT_ROOT / 'docs' / 'reports' / 'FINAL_RESULTS.md'\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'outputs' / 'training_audiomae'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Biblioth√®ques import√©es\")\n",
    "print(f\"üìÅ Projet : {PROJECT_ROOT}\")\n",
    "print(f\"üîß PyTorch : {torch.__version__}\")\n",
    "print(f\"üéÆ CUDA : {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Architecture Vision Transformer AudioMAE {#2-architecture}\n",
    "\n",
    "### Structure Compl√®te\n",
    "\n",
    "AudioMAE adapte l'architecture Vision Transformer (ViT) pour l'audio :\n",
    "\n",
    "```\n",
    "Input: Mel Spectrogramme (1, 128, 128)\n",
    "  ‚Üì\n",
    "Patch Embedding (16√ó16) ‚Üí 64 patches\n",
    "  ‚Üì\n",
    "Position Encoding (learnable)\n",
    "  ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Vision Transformer Encoder         ‚îÇ\n",
    "‚îÇ  - 12 couches transformer           ‚îÇ\n",
    "‚îÇ  - 768 embedding dimension          ‚îÇ\n",
    "‚îÇ  - 12 attention heads               ‚îÇ\n",
    "‚îÇ  - MLP ratio 4√ó (3072 hidden)      ‚îÇ\n",
    "‚îÇ  - Dropout: 0.0                     ‚îÇ\n",
    "‚îÇ  - Attention dropout: 0.0           ‚îÇ\n",
    "‚îÇ  - Drop path: 0.1                   ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "  ‚Üì\n",
    "Layer Norm ‚Üí (768 features)\n",
    "  ‚Üì\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  Classification Head                ‚îÇ\n",
    "‚îÇ  - Dense(768) ‚Üí GELU               ‚îÇ\n",
    "‚îÇ  - Dropout(0.5)                     ‚îÇ\n",
    "‚îÇ  - Dense(7) ‚Üí Softmax              ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "### Composants Cl√©s\n",
    "\n",
    "**1. Patch Embedding**\n",
    "- Divise le spectrogramme 128√ó128 en patches 16√ó16\n",
    "- R√©sultat : 64 patches (8√ó8 grid)\n",
    "- Chaque patch ‚Üí embedding 768-dim\n",
    "\n",
    "**2. Transformer Encoder**\n",
    "- **12 couches** de transformer blocks\n",
    "- **Multi-Head Self-Attention** : 12 heads\n",
    "- **MLP Feed-Forward** : 768 ‚Üí 3072 ‚Üí 768\n",
    "- **Residual Connections** + Layer Norm\n",
    "- **Drop Path** : Stochastic depth (0.1)\n",
    "\n",
    "**3. Classification Head**\n",
    "- Dense layer avec forte r√©gularisation\n",
    "- Dropout 0.5 pour √©viter overfitting\n",
    "- Output : 7 classes (v√©hicules militaires)\n",
    "\n",
    "### Input : Mel Spectrogramme 10 secondes\n",
    "\n",
    "**Param√®tres audio** :\n",
    "- **Sample rate** : 16,000 Hz\n",
    "- **Dur√©e** : 10.0 secondes (160,000 samples)\n",
    "- **n_fft** : 1024\n",
    "- **hop_length** : 160 (10ms frames)\n",
    "- **n_mels** : 128 bandes mel\n",
    "- **f_min / f_max** : 50 Hz / 8000 Hz\n",
    "- **Shape brute** : (128, 1000)\n",
    "- **Shape finale** : (128, 128) apr√®s resize\n",
    "\n",
    "### Param√®tres Totaux\n",
    "\n",
    "**111,089,927 param√®tres** (~424 MB FP32)\n",
    "\n",
    "**D√©composition** :\n",
    "- Patch embedding : ~600K\n",
    "- Transformer encoder : ~110M\n",
    "- Classification head : ~500K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement de la configuration\n",
    "print(\"üìÑ Chargement de la configuration AudioMAE...\\n\")\n",
    "\n",
    "if TRAINING_CONFIG.exists():\n",
    "    with open(TRAINING_CONFIG, 'r') as f:\n",
    "        train_config = json.load(f)\n",
    "    \n",
    "    model_cfg = train_config.get('model', {})\n",
    "    arch = model_cfg.get('architecture', {})\n",
    "    encoder = arch.get('encoder', {})\n",
    "    decoder = arch.get('decoder', {})\n",
    "    \n",
    "    print(\"üèóÔ∏è Architecture AudioMAE :\")\n",
    "    print(f\"   ‚Ä¢ Patch size        : {encoder.get('patch_size')}\")\n",
    "    print(f\"   ‚Ä¢ Embed dim         : {encoder.get('embed_dim')}\")\n",
    "    print(f\"   ‚Ä¢ Encoder depth     : {encoder.get('depth')} couches\")\n",
    "    print(f\"   ‚Ä¢ Attention heads   : {encoder.get('num_heads')}\")\n",
    "    print(f\"   ‚Ä¢ MLP ratio         : {encoder.get('mlp_ratio')}√ó\")\n",
    "    print(f\"   ‚Ä¢ Dropout           : {encoder.get('dropout')}\")\n",
    "    print(f\"   ‚Ä¢ Drop path         : {encoder.get('drop_path')}\")\n",
    "    \n",
    "    spec_cfg = model_cfg.get('audio', {}).get('spectrogram', {})\n",
    "    print(\"\\nüéµ Configuration Spectrogramme :\")\n",
    "    for key, value in spec_cfg.items():\n",
    "        print(f\"   ‚Ä¢ {key:15s} : {value}\")\n",
    "    \n",
    "    classif = model_cfg.get('classification', {})\n",
    "    print(f\"\\nüéØ Classification :\")\n",
    "    print(f\"   ‚Ä¢ Num classes       : {classif.get('num_classes')}\")\n",
    "    print(f\"   ‚Ä¢ Dropout           : {classif.get('dropout')}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Configuration non trouv√©e : {TRAINING_CONFIG}\")\n",
    "    train_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D√©finition simplifi√©e de l'architecture\n",
    "print(\"\\nüìä Sp√©cifications du mod√®le AudioMAE :\\n\")\n",
    "\n",
    "specs = {\n",
    "    'Input Shape': '(1, 128, 128)',\n",
    "    'Patch Size': '16√ó16',\n",
    "    'Num Patches': '64 (8√ó8)',\n",
    "    'Embed Dimension': '768',\n",
    "    'Encoder Layers': '12',\n",
    "    'Attention Heads': '12',\n",
    "    'MLP Hidden': '3072 (4√ó embed)',\n",
    "    'Total Parameters': '111,089,927',\n",
    "    'Model Size (FP32)': '~424 MB',\n",
    "    'Model Size (INT8)': '~83 MB (quantized)',\n",
    "    'Audio Duration': '10 seconds',\n",
    "    'Context vs CNN': '3.3√ó longer',\n",
    "    'Context vs CRNN': '2.5√ó longer'\n",
    "}\n",
    "\n",
    "for key, value in specs.items():\n",
    "    print(f\"   {key:25s} : {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° AudioMAE est 459√ó plus grand que CNN et 74√ó plus grand que CRNN\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Comparaison avec CNN et CRNN {#3-comparaison}\n",
    "\n",
    "### Tableau Comparatif Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison exhaustive des 3 mod√®les\n",
    "comparison = {\n",
    "    'Aspect': [\n",
    "        'Architecture',\n",
    "        'Param√®tres',\n",
    "        'Taille Mod√®le',\n",
    "        'Features Input',\n",
    "        'Input Shape',\n",
    "        'Dur√©e Audio',\n",
    "        'Sample Rate',\n",
    "        'Best Val Acc',\n",
    "        'Final Val Acc',\n",
    "        'Train Acc (final)',\n",
    "        'G√©n√©ralisation',\n",
    "        'Training Time',\n",
    "        'Batch Size',\n",
    "        'Epochs',\n",
    "        'Best Epoch'\n",
    "    ],\n",
    "    'CNN-MFCC': [\n",
    "        '3 Conv + Dense',\n",
    "        '242K',\n",
    "        '~1 MB',\n",
    "        'MFCC 40 coef',\n",
    "        '(3, 40, 92)',\n",
    "        '3 secondes',\n",
    "        '16kHz',\n",
    "        '66.88%',\n",
    "        '57.95%',\n",
    "        '76.52%',\n",
    "        '-8.93% (overfit)',\n",
    "        '2-3 heures',\n",
    "        '32',\n",
    "        '150',\n",
    "        '29'\n",
    "    ],\n",
    "    'CRNN-MFCC': [\n",
    "        '3 Conv + 2 BiLSTM',\n",
    "        '1.5M',\n",
    "        '~6 MB',\n",
    "        'MFCC 40 coef',\n",
    "        '(3, 40, 124)',\n",
    "        '4 secondes',\n",
    "        '16kHz',\n",
    "        '73.21%',\n",
    "        '72.32%',\n",
    "        '79.07%',\n",
    "        '-0.89% (stable)',\n",
    "        '5-6 heures',\n",
    "        '24',\n",
    "        '100',\n",
    "        '47'\n",
    "    ],\n",
    "    'AudioMAE': [\n",
    "        'ViT (12 layers)',\n",
    "        '111M',\n",
    "        '~424 MB',\n",
    "        'Mel Spec 128',\n",
    "        '(1, 128, 128)',\n",
    "        '10 secondes',\n",
    "        '16kHz',\n",
    "        '82.15%',\n",
    "        '82.15%',\n",
    "        '69.77%',\n",
    "        '+12.38% (excellent!)',\n",
    "        '~4 heures',\n",
    "        '16',\n",
    "        '100',\n",
    "        '0 (early)'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comp = pd.DataFrame(comparison)\n",
    "\n",
    "print(\"üìä COMPARAISON COMPL√àTE : CNN vs CRNN vs AudioMAE\\n\")\n",
    "print(df_comp.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ AM√âLIORATIONS AUDIOMAE :\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  vs CNN  : +15.20% accuracy (82.15% vs 66.88%)\")\n",
    "print(f\"  vs CRNN : +8.94% accuracy (82.15% vs 73.21%)\")\n",
    "print(f\"  G√©n√©ralisation EXCEPTIONNELLE : Val Acc > Train Acc (+12.38%)\")\n",
    "print(f\"  Aucun overfitting observ√© sur 100 epochs\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Configuration d'Entra√Ænement {#4-configuration}\n",
    "\n",
    "### Hyperparam√®tres Avanc√©s\n",
    "\n",
    "AudioMAE utilise une configuration d'entra√Ænement sophistiqu√©e :\n",
    "\n",
    "**Optimizer : AdamW**\n",
    "- Learning Rate : 1e-4 (0.0001)\n",
    "- Weight Decay : 0.05 (forte r√©gularisation)\n",
    "- Betas : (0.9, 0.95)\n",
    "- Epsilon : 1e-8\n",
    "\n",
    "**LR Scheduler : Cosine Annealing with Warm Restarts**\n",
    "- T_0 : 10 epochs (restart period)\n",
    "- T_mult : 2 (period multiplication)\n",
    "- Min LR : 1e-6\n",
    "- Warmup : 1000 steps\n",
    "\n",
    "**Training**\n",
    "- Batch Size : 16 (limit√© par m√©moire GPU)\n",
    "- Epochs : 100\n",
    "- Gradient Clipping : 1.0\n",
    "- Mixed Precision : False (FP32)\n",
    "\n",
    "**Regularization Forte**\n",
    "- **Mixup** : Alpha 0.8 (m√©lange d'exemples)\n",
    "- **CutMix** : Alpha 1.0 (d√©coupe et m√©lange)\n",
    "- **Probability** : 0.5 (appliqu√© 50% du temps)\n",
    "- **Label Smoothing** : 0.1\n",
    "- **Classifier Dropout** : 0.5\n",
    "- **Drop Path** : 0.1 (stochastic depth)\n",
    "\n",
    "**Loss Function**\n",
    "- CrossEntropyLoss avec label smoothing\n",
    "\n",
    "### Commande d'Entra√Ænement\n",
    "\n",
    "```bash\n",
    "python scripts/train_model.py \\\n",
    "    --config configs/models/audioMAE.yaml \\\n",
    "    --data-dir data/raw/mad \\\n",
    "    --epochs 100 \\\n",
    "    --batch-size 16 \\\n",
    "    --learning-rate 1e-4 \\\n",
    "    --weight-decay 0.05 \\\n",
    "    --warmup-steps 1000 \\\n",
    "    --mixup-alpha 0.8 \\\n",
    "    --cutmix-alpha 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration d√©taill√©e\n",
    "if train_config:\n",
    "    training_cfg = train_config.get('training', {})\n",
    "    optim_cfg = training_cfg.get('optimizer', {})\n",
    "    sched_cfg = training_cfg.get('scheduler', {})\n",
    "    aug_cfg = training_cfg.get('augmentation', {})\n",
    "    \n",
    "    print(\"‚öôÔ∏è CONFIGURATION D'ENTRA√éNEMENT AUDIOMAE\\n\")\n",
    "    \n",
    "    print(\"üìà Optimizer (AdamW) :\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate     : {optim_cfg.get('learning_rate')}\")\n",
    "    print(f\"   ‚Ä¢ Weight Decay      : {optim_cfg.get('weight_decay')}\")\n",
    "    print(f\"   ‚Ä¢ Betas             : {optim_cfg.get('betas')}\")\n",
    "    \n",
    "    print(\"\\nüìä Scheduler (Cosine) :\")\n",
    "    print(f\"   ‚Ä¢ Type              : {sched_cfg.get('type')}\")\n",
    "    print(f\"   ‚Ä¢ T_0               : {sched_cfg.get('T_0')} epochs\")\n",
    "    print(f\"   ‚Ä¢ T_mult            : {sched_cfg.get('T_mult')}\")\n",
    "    print(f\"   ‚Ä¢ Min LR            : {sched_cfg.get('min_lr')}\")\n",
    "    print(f\"   ‚Ä¢ Warmup steps      : {sched_cfg.get('warmup_steps')}\")\n",
    "    \n",
    "    print(\"\\nüé® Augmentation :\")\n",
    "    mixup = aug_cfg.get('mixup', {})\n",
    "    print(f\"   ‚Ä¢ Mixup enabled     : {mixup.get('enabled')}\")\n",
    "    print(f\"   ‚Ä¢ Mixup alpha       : {mixup.get('alpha')}\")\n",
    "    print(f\"   ‚Ä¢ CutMix alpha      : {mixup.get('cutmix_alpha')}\")\n",
    "    print(f\"   ‚Ä¢ Probability       : {mixup.get('probability')}\")\n",
    "    print(f\"   ‚Ä¢ Label smoothing   : {aug_cfg.get('label_smoothing')}\")\n",
    "    \n",
    "    print(\"\\nüîß Training :\")\n",
    "    print(f\"   ‚Ä¢ Epochs            : {training_cfg.get('epochs')}\")\n",
    "    print(f\"   ‚Ä¢ Batch size        : {training_cfg.get('batch_size')}\")\n",
    "    print(f\"   ‚Ä¢ Gradient clip     : {training_cfg.get('gradient_clip')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Processus d'Entra√Ænement {#5-entrainement}\n",
    "\n",
    "### D√©tails de l'Entra√Ænement\n",
    "\n",
    "**Dur√©e totale** : 237.7 minutes (3h 57min)\n",
    "\n",
    "**Progression par epochs** :\n",
    "- **Epochs 1-10** : Warmup + apprentissage initial\n",
    "- **Epochs 11-50** : Am√©lioration continue\n",
    "- **Epochs 51-100** : Fine-tuning et stabilisation\n",
    "\n",
    "**Observations cl√©s** :\n",
    "1. **Best epoch** : 0 (early convergence)\n",
    "2. **Validation > Training** : Ph√©nom√®ne unique!\n",
    "3. **Stable sur 100 epochs** : Pas de d√©gradation\n",
    "4. **Training loss > Val loss** : R√©gularisation forte\n",
    "\n",
    "### M√©triques R√©centes (Epochs 98-100)\n",
    "\n",
    "```\n",
    "Epoch 98/100:\n",
    "  Train Loss: 0.9563  Train Acc: 71.27%\n",
    "  \n",
    "Epoch 99/100:\n",
    "  Train Loss: 0.9372  Train Acc: 70.49%\n",
    "  \n",
    "Epoch 100/100:\n",
    "  Train Loss: 0.9763  Train Acc: 69.77%\n",
    "  Val Loss: 0.8693    Val Acc: 82.15%\n",
    "```\n",
    "\n",
    "### Temps d'Entra√Ænement\n",
    "\n",
    "- **Total** : 237.7 minutes (~4 heures)\n",
    "- **Par epoch** : ~2.4 minutes\n",
    "- **Vs CNN** : Similaire (2-3h pour 150 epochs)\n",
    "- **Vs CRNN** : Plus rapide (5-6h pour 100 epochs)\n",
    "\n",
    "**Raison** : Bien que 111M params, AudioMAE b√©n√©ficie de :\n",
    "- Optimisations GPU pour transformers\n",
    "- Batch size adapt√© (16)\n",
    "- Moins d'epochs n√©cessaires (100 vs 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. R√©sultats Exceptionnels {#6-resultats}\n",
    "\n",
    "### M√©triques Finales (Epoch 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sultats finaux AudioMAE\n",
    "results = {\n",
    "    'M√©trique': [\n",
    "        'Validation Accuracy',\n",
    "        'Training Accuracy',\n",
    "        'Validation Loss',\n",
    "        'Training Loss',\n",
    "        'G√©n√©ralisation Gap',\n",
    "        'Best Epoch',\n",
    "        'Final Epoch',\n",
    "        'Training Time',\n",
    "        'Performance Grade'\n",
    "    ],\n",
    "    'Valeur': [\n",
    "        '82.15%',\n",
    "        '69.77%',\n",
    "        '0.8693',\n",
    "        '0.9763',\n",
    "        '+12.38%',\n",
    "        '0',\n",
    "        '100',\n",
    "        '237.7 min',\n",
    "        'A- / B+'\n",
    "    ],\n",
    "    'Comparaison': [\n",
    "        '+15.2% vs CNN',\n",
    "        'Plus bas (r√©gularisation)',\n",
    "        'Excellent',\n",
    "        'Plus haut (mixup)',\n",
    "        'Val > Train (unique!)',\n",
    "        'Early convergence',\n",
    "        'Stable',\n",
    "        '2√ó CNN, 0.7√ó CRNN',\n",
    "        'Meilleur mod√®le'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\" + \"=\"*80)\n",
    "print(\" \"*20 + \"üèÜ R√âSULTATS FINAUX AUDIOMAE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ POINTS CL√âS :\")\n",
    "print(\"=\"*80)\n",
    "print(\"  ‚úÖ Validation Accuracy : 82.15% (RECORD du projet)\")\n",
    "print(\"  ‚úÖ G√©n√©ralisation exceptionnelle : Val Acc > Train Acc (+12.38%)\")\n",
    "print(\"  ‚úÖ Stable sur 100 epochs : Aucune d√©gradation\")\n",
    "print(\"  ‚úÖ Am√©lioration majeure : +15.2% vs CNN baseline\")\n",
    "print(\"  ‚úÖ Performance grade : A- / B+\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Analyse de la G√©n√©ralisation {#7-generalisation}\n",
    "\n",
    "### Ph√©nom√®ne Unique : Val Acc > Train Acc\n",
    "\n",
    "**Observation extraordinaire** :\n",
    "- Training Accuracy : 69.77%\n",
    "- Validation Accuracy : 82.15%\n",
    "- **Gap : +12.38%** (validation MEILLEURE que training!)\n",
    "\n",
    "### Pourquoi ce ph√©nom√®ne ?\n",
    "\n",
    "Ce r√©sultat contre-intuitif s'explique par :\n",
    "\n",
    "**1. R√©gularisation Forte pendant Training**\n",
    "- **Mixup** (Œ±=0.8) : M√©lange fortement les exemples\n",
    "- **CutMix** (Œ±=1.0) : D√©coupe et combine des r√©gions\n",
    "- **Label Smoothing** (0.1) : R√©duit la confiance\n",
    "- **Effect** : Training devient artificiellement PLUS DUR\n",
    "\n",
    "**2. Pas d'Augmentation sur Validation**\n",
    "- Validation : Spectrogrammes propres, non modifi√©s\n",
    "- Training : Spectrogrammes fortement augment√©s\n",
    "- **Effect** : Validation est PLUS FACILE\n",
    "\n",
    "**3. Capacit√© du Mod√®le**\n",
    "- 111M param√®tres : Grande capacit√© d'apprentissage\n",
    "- Transformers : Apprennent des repr√©sentations robustes\n",
    "- **Effect** : G√©n√©ralise mieux que memorise\n",
    "\n",
    "**4. Dropout Fort en Inf√©rence OFF**\n",
    "- Training : Dropout 0.5 actif\n",
    "- Validation : Dropout d√©sactiv√©\n",
    "- **Effect** : Mod√®le complet utilis√© en validation\n",
    "\n",
    "### C'est un BON Signe!\n",
    "\n",
    "Ce ph√©nom√®ne indique :\n",
    "- ‚úÖ **Excellente g√©n√©ralisation** : Pas d'overfitting\n",
    "- ‚úÖ **R√©gularisation efficace** : Mod√®le robuste\n",
    "- ‚úÖ **Capacit√© bien utilis√©e** : 111M params exploit√©s\n",
    "- ‚úÖ **Augmentation pertinente** : Training plus difficile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la g√©n√©ralisation\n",
    "models_gen = ['CNN', 'CRNN', 'AudioMAE']\n",
    "train_accs = [76.52, 79.07, 69.77]\n",
    "val_accs = [66.88, 73.21, 82.15]\n",
    "gaps = [val - train for val, train in zip(val_accs, train_accs)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 1. Train vs Val Accuracy\n",
    "x = np.arange(len(models_gen))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, train_accs, width, label='Train Acc', \n",
    "                    color='coral', edgecolor='black', linewidth=2)\n",
    "bars2 = axes[0].bar(x + width/2, val_accs, width, label='Val Acc', \n",
    "                    color='forestgreen', edgecolor='black', linewidth=2)\n",
    "\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "axes[0].set_title('Train vs Validation Accuracy', fontsize=15, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models_gen, fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "axes[0].set_ylim([0, 90])\n",
    "\n",
    "# Ajouter valeurs\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Generalization Gap\n",
    "colors_gap = ['red' if g < -5 else 'orange' if g < 0 else 'green' for g in gaps]\n",
    "bars = axes[1].bar(models_gen, gaps, color=colors_gap, edgecolor='black', linewidth=2)\n",
    "axes[1].axhline(y=0, color='black', linestyle='-', linewidth=2)\n",
    "axes[1].set_ylabel('Gap (%)', fontsize=13, fontweight='bold')\n",
    "axes[1].set_title('G√©n√©ralisation (Val - Train)', fontsize=15, fontweight='bold')\n",
    "axes[1].set_xticklabels(models_gen, fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter valeurs\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    label = f'{height:+.2f}%'\n",
    "    if gaps[i] > 0:\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                    label, ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    else:\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., height - 0.5,\n",
    "                    label, ha='center', va='top', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / 'audiomae_generalization.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Graphique sauvegard√© : audiomae_generalization.png\")\n",
    "\n",
    "print(\"\\nüéØ Analyse :\")\n",
    "print(f\"   CNN   : {gaps[0]:.2f}% (Overfitting s√©v√®re)\")\n",
    "print(f\"   CRNN  : {gaps[1]:.2f}% (L√©g√®re d√©gradation)\")\n",
    "print(f\"   AudioMAE : {gaps[2]:.2f}% (G√©n√©ralisation EXCELLENTE!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Visualisations Compl√®tes {#8-visualisations}\n",
    "\n",
    "### Comparaison Finale des 3 Mod√®les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle finale\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "models = ['CNN', 'CRNN', 'AudioMAE']\n",
    "best_accs = [66.88, 73.21, 82.15]\n",
    "final_accs = [57.95, 72.32, 82.15]\n",
    "params = [242, 1500, 111000]\n",
    "times = [2.5, 5.5, 4.0]\n",
    "\n",
    "# 1. Best Accuracy\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "bars = ax1.bar(models, best_accs, color=['steelblue', 'darkorange', 'forestgreen'], \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax1.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax1.set_title('Best Validation Accuracy', fontweight='bold', fontsize=13)\n",
    "ax1.set_ylim([0, 90])\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars, best_accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., acc + 2,\n",
    "            f'{acc:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Final Accuracy\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "bars = ax2.bar(models, final_accs, color=['coral', 'gold', 'limegreen'], \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "ax2.set_title('Final Validation Accuracy', fontweight='bold', fontsize=13)\n",
    "ax2.set_ylim([0, 90])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, acc in zip(bars, final_accs):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., acc + 2,\n",
    "            f'{acc:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Parameters (log scale)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "bars = ax3.bar(models, params, color=['lightblue', 'lightyellow', 'lightcoral'], \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax3.set_ylabel('Param√®tres (K)', fontweight='bold')\n",
    "ax3.set_title('Nombre de Param√®tres', fontweight='bold', fontsize=13)\n",
    "ax3.set_yscale('log')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "for bar, p in zip(bars, params):\n",
    "    if p < 1000:\n",
    "        label = f'{p}K'\n",
    "    else:\n",
    "        label = f'{p/1000:.0f}M'\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., p * 1.2,\n",
    "            label, ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Training Time\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "bars = ax4.bar(models, times, color=['plum', 'khaki', 'palegreen'], \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax4.set_ylabel('Heures', fontweight='bold')\n",
    "ax4.set_title('Temps d\\'Entra√Ænement', fontweight='bold', fontsize=13)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "for bar, t in zip(bars, times):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., t + 0.2,\n",
    "            f'{t:.1f}h', ha='center', fontweight='bold')\n",
    "\n",
    "# 5. Improvement vs CNN\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "improvements = [0, 6.33, 15.27]\n",
    "bars = ax5.bar(models, improvements, color=['gray', 'orange', 'green'], \n",
    "               edgecolor='black', linewidth=2)\n",
    "ax5.set_ylabel('Am√©lioration (%)', fontweight='bold')\n",
    "ax5.set_title('Am√©lioration vs CNN Baseline', fontweight='bold', fontsize=13)\n",
    "ax5.grid(True, alpha=0.3, axis='y')\n",
    "for bar, imp in zip(bars, improvements):\n",
    "    ax5.text(bar.get_x() + bar.get_width()/2., imp + 0.5,\n",
    "            f'+{imp:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "# 6. Overfitting Score\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "overfits = [8.93, 0.89, -12.38]\n",
    "colors_over = ['red', 'orange', 'green']\n",
    "bars = ax6.bar(models, overfits, color=colors_over, edgecolor='black', linewidth=2)\n",
    "ax6.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax6.set_ylabel('Gap (%)', fontweight='bold')\n",
    "ax6.set_title('Overfitting (n√©gatif = bon)', fontweight='bold', fontsize=13)\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "for bar, over in zip(bars, overfits):\n",
    "    height = bar.get_height()\n",
    "    if over > 0:\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'+{over:.1f}%', ha='center', fontweight='bold', color='red')\n",
    "    else:\n",
    "        ax6.text(bar.get_x() + bar.get_width()/2., height - 1,\n",
    "                f'{over:.1f}%', ha='center', fontweight='bold', color='green')\n",
    "\n",
    "# 7. Final Summary Table\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "ax7.axis('off')\n",
    "\n",
    "summary_text = \"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    üèÜ R√âSUM√â COMPARATIF FINAL                                ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üìä AUDIOMAE : MEILLEUR MOD√àLE DU PROJET                                    ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  ‚úÖ Accuracy Record        : 82.15% (+15.27% vs CNN)                        ‚ïë\n",
    "‚ïë  ‚úÖ G√©n√©ralisation         : EXCEPTIONNELLE (Val > Train)                   ‚ïë\n",
    "‚ïë  ‚úÖ Stabilit√©              : 100 epochs sans d√©gradation                    ‚ïë\n",
    "‚ïë  ‚úÖ Architecture           : Vision Transformer (state-of-the-art)          ‚ïë\n",
    "‚ïë  ‚úÖ Contexte               : 10 secondes (3.3√ó CNN, 2.5√ó CRNN)             ‚ïë\n",
    "‚ïë  ‚úÖ Production-ready       : Quantization INT8 disponible                   ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïë  üéØ Performance Grade      : A- / B+                                         ‚ïë\n",
    "‚ïë  üöÄ D√©ploy√© sur RPi5       : 260-340ms latence                              ‚ïë\n",
    "‚ïë                                                                              ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\"\n",
    "\n",
    "ax7.text(0.5, 0.5, summary_text, ha='center', va='center',\n",
    "        fontsize=10, family='monospace', fontweight='bold',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "plt.savefig(OUTPUT_DIR / 'final_comparison_all_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üíæ Graphique sauvegard√© : final_comparison_all_models.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Conclusion et Impact {#9-conclusion}\n",
    "\n",
    "### R√©sum√© AudioMAE\n",
    "\n",
    "**‚úÖ R√©alisations Majeures** :\n",
    "1. **Accuracy record** : 82.15% (meilleur du projet)\n",
    "2. **G√©n√©ralisation exceptionnelle** : Val > Train (+12.38%)\n",
    "3. **Architecture moderne** : Vision Transformer\n",
    "4. **Stable** : 100 epochs sans d√©gradation\n",
    "5. **Contexte long** : 10 secondes d'audio\n",
    "6. **Production-ready** : Quantization INT8 (-75% taille)\n",
    "\n",
    "**üìä Comparaison Finale** :\n",
    "\n",
    "| M√©trique | CNN | CRNN | AudioMAE | Am√©lioration |\n",
    "|----------|-----|------|----------|-------------|\n",
    "| **Best Acc** | 66.88% | 73.21% | **82.15%** | **+15.27%** |\n",
    "| **Final Acc** | 57.95% | 72.32% | **82.15%** | **+24.20%** |\n",
    "| **Overfitting** | -8.93% | -0.89% | **+12.38%** | **Aucun!** |\n",
    "| **Params** | 242K | 1.5M | 111M | 459√ó CNN |\n",
    "| **Training Time** | 2-3h | 5-6h | 4h | Optimal |\n",
    "\n",
    "### Impact du Projet\n",
    "\n",
    "**üéØ Objectif atteint** :\n",
    "- D√©tection de v√©hicules militaires : ‚úÖ 82.15%\n",
    "- D√©ploiement edge (RPi5) : ‚úÖ 260-340ms\n",
    "- Production-ready : ‚úÖ INT8 quantization\n",
    "\n",
    "**üî¨ Contributions** :\n",
    "1. D√©monstration de l'efficacit√© des transformers pour l'audio\n",
    "2. Pipeline complet data ‚Üí training ‚Üí deployment\n",
    "3. Comparaison exhaustive CNN vs CRNN vs Transformer\n",
    "4. Optimisation edge avec quantization INT8\n",
    "\n",
    "### Limitations\n",
    "\n",
    "**‚ö†Ô∏è Consid√©rations** :\n",
    "1. **Mod√®le lourd** : 111M params (424 MB FP32)\n",
    "2. **M√©moire GPU** : Batch size limit√© √† 16\n",
    "3. **Complexit√©** : Plus difficile √† interpr√©ter\n",
    "4. **Inference** : Plus lent que CNN/CRNN\n",
    "5. **Quantization** : Perte minime (-0.28%) mais n√©cessaire\n",
    "\n",
    "### Recommandations\n",
    "\n",
    "**Quand utiliser AudioMAE** :\n",
    "- ‚úÖ Accuracy critique (production)\n",
    "- ‚úÖ Ressources GPU disponibles\n",
    "- ‚úÖ Latence <500ms acceptable\n",
    "- ‚úÖ Contexte long important\n",
    "\n",
    "**Alternatives** :\n",
    "- **CRNN** : Balance accuracy/taille (73.21%, 6MB)\n",
    "- **CNN** : Ultra-l√©ger pour edge (<1MB)\n",
    "\n",
    "### Prochaines √âtapes\n",
    "\n",
    "Le **Notebook 5** documentera :\n",
    "- Export ONNX du mod√®le\n",
    "- Quantization INT8 (424 MB ‚Üí 83 MB)\n",
    "- D√©ploiement Raspberry Pi 5\n",
    "- Tests de performance edge\n",
    "- Optimisations inference\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center; padding: 30px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white;\">\n",
    "    <h2>üèÜ Notebook 4 Compl√©t√© !</h2>\n",
    "    <h3>AudioMAE : Champion du Projet SereneSense</h3>\n",
    "    <p style=\"font-size: 20px; margin: 20px 0;\">\n",
    "        <b>82.15% Accuracy</b> | <b>111M Parameters</b> | <b>Vision Transformer</b>\n",
    "    </p>\n",
    "    <p style=\"font-size: 18px;\">\n",
    "        <b>+15.27% vs CNN | +8.94% vs CRNN</b>\n",
    "    </p>\n",
    "    <p style=\"font-size: 16px; margin-top: 20px;\">\n",
    "        G√©n√©ralisation Exceptionnelle : Val Acc > Train Acc (+12.38%)\n",
    "    </p>\n",
    "    <p style=\"font-size: 14px; margin-top: 15px; font-style: italic;\">\n",
    "        Performance Grade: A- / B+ | Production-Ready\n",
    "    </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
